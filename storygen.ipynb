{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fZPI0rXy2aD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "# from google.colab import files, drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2lPn414ZZ-T"
      },
      "outputs": [],
      "source": [
        "# path to the original uncleaned short story data file\n",
        "short_stories_filename = \"reddit_short_stories.txt\"\n",
        "# hyperparameters\n",
        "hidden_size = 128\n",
        "n_layers = 3\n",
        "n_grams_size = 5\n",
        "num_epochs = 15\n",
        "learning_rate = 0.015\n",
        "batch_size = 128\n",
        "# define length of story to generate\n",
        "generation_length = 20\n",
        "# define input story prompt\n",
        "prompt = 'family decided spend time around city'\n",
        "# change below to some path if weight storing is desired\n",
        "# otherwise leave it as None\n",
        "checkpoint_path = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJo7RSLLy2aF",
        "outputId": "b1a06240-176c-499f-9056-ca4a9b0bbbd9"
      },
      "outputs": [],
      "source": [
        "nltk.download(['stopwords', 'wordnet', 'punkt', ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bGnc5wfy2aF"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.is_available()\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4otQjYTy2aG"
      },
      "outputs": [],
      "source": [
        "with open(short_stories_filename, 'r') as file:\n",
        "    data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG9MYRWGy2aG"
      },
      "outputs": [],
      "source": [
        "# Split the data string into a list of stories\n",
        "stories = data.split(\"<eos>\\n<sos>\")\n",
        "\n",
        "# Remove the first <sos> tag from the first story\n",
        "stories[0] = stories[0].replace(\"<sos>\", \"\")\n",
        "\n",
        "# Remove the last <eos> tag from the last story\n",
        "stories[-1] = stories[-1].replace(\"<eos>\", \"\")\n",
        "\n",
        "# Create a dataframe with one story per row\n",
        "reddit_df = pd.DataFrame(stories, columns=['contents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQpgjR5Py2aG"
      },
      "outputs": [],
      "source": [
        "all_sentences = []\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[{}]+'.format(string.punctuation), ' ', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "    return pattern.sub('', text)\n",
        "\n",
        "def strip_spaces(text):\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "    tokens = nltk.tokenize.word_tokenize(sentence)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "def clean_sentence(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = remove_punctuation(text)\n",
        "    \n",
        "    # Remove stop words\n",
        "    text = remove_stopwords(text.lower())\n",
        "\n",
        "    # Lemmatize words in the sentence\n",
        "    text = lemmatize_sentence(text)\n",
        "\n",
        "    # Strip spaces\n",
        "    text = strip_spaces(text)\n",
        "\n",
        "    # add the sentence to the list of all sentences to be used later for BLEU\n",
        "    all_sentences.append(text.split(\" \"))\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_ngrams(sentence, n_grams):\n",
        "    tokenized = nltk.word_tokenize(sentence.lower())\n",
        "    return list(ngrams(tokenized, n_grams))\n",
        "\n",
        "def clean_story(text):\n",
        "    sentences = []\n",
        "    for sentence in text.split('.'):\n",
        "        if sentence:\n",
        "            sentences.append(clean_sentence(sentence))\n",
        "    return sentences\n",
        "\n",
        "def generate_n_grams(text, n_grams: int = 3):\n",
        "    arr = []\n",
        "    for sentence in text:\n",
        "        arr += create_ngrams(sentence, n_grams)\n",
        "    return arr "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALq57xb1y2aH"
      },
      "outputs": [],
      "source": [
        "# Decrease amount of stories as dataset is very large\n",
        "reddit_df = reddit_df.iloc[500:1500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpIlPH3cy2aH"
      },
      "outputs": [],
      "source": [
        "# Perform story cleaning\n",
        "reddit_df['sentences'] = reddit_df['contents'].apply(lambda text: clean_story(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmn6furVy2aI"
      },
      "outputs": [],
      "source": [
        "# Generate ngrams\n",
        "reddit_df['n_grams'] = reddit_df['sentences'].apply(lambda text: generate_n_grams(text, n_grams=n_grams_size))\n",
        "\n",
        "# Make sure there are no empty arrays in df\n",
        "fixed_n_grams = []\n",
        "for arr in reddit_df['n_grams']:\n",
        "  if arr != []:\n",
        "    fixed_n_grams.append(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUXEkcqoy2aI"
      },
      "outputs": [],
      "source": [
        "n_grams = np.concatenate(fixed_n_grams).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV8A89Wuy2aI"
      },
      "outputs": [],
      "source": [
        "# Make a single array of all words from ngrams\n",
        "split_sentences = []\n",
        "for arr in n_grams:\n",
        "    split_sentences += arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ-aDHK0y2aJ"
      },
      "outputs": [],
      "source": [
        "def create_vocabulary(sentence):\n",
        "    # Create empty dictionaries\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "\n",
        "    # Get array of set of words\n",
        "    word_set = list(set(sentence))\n",
        "\n",
        "    # Iterate over set of words and save them to dictionaries\n",
        "    for i in range(len(word_set)):\n",
        "        word_to_index[word_set[i]] = i\n",
        "        index_to_word[i] = word_set[i]\n",
        "\n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "\n",
        "def input_target_generator(n_grams: list, word_to_index: dict):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # Iterate over ngrams\n",
        "    for i in range(len(n_grams)):\n",
        "\n",
        "        # Separate input from ngram and target\n",
        "        input_part = n_grams[i][:-1]\n",
        "        target_word = n_grams[i][-1]\n",
        "\n",
        "        # Create and append input and target\n",
        "        _input = []\n",
        "        _target = []\n",
        "        for word in input_part:\n",
        "            _input.append(word_to_index[word])\n",
        "        _target.append(word_to_index[target_word])\n",
        "\n",
        "        # Convert to torch arrays\n",
        "        _input = torch.tensor(_input, dtype=torch.long)\n",
        "        _target = torch.tensor(_target, dtype=torch.long)\n",
        "\n",
        "        inputs.append(_input)\n",
        "        targets.append(_target)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "# Create vocabulary and 'reverse' vocabulary\n",
        "word_to_index, index_to_word = create_vocabulary(sentence=split_sentences)\n",
        "# Create arrays of inputs and targets\n",
        "inputs, targets = input_target_generator(n_grams=n_grams, word_to_index=word_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yLdZFXFT23s"
      },
      "outputs": [],
      "source": [
        "# Join sentences in each Story then join all stories\n",
        "joined_sentences = [' '.join(sentences) for sentences in reddit_df['sentences']]\n",
        "all_stories = ' '.join(joined_sentences)\n",
        "\n",
        "# Tokenize the words\n",
        "words = nltk.word_tokenize(all_stories)\n",
        "\n",
        "# Count the occurrences of each word\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Top 10 most occuring words\n",
        "top_10_words = word_counts.most_common(10)\n",
        "print(top_10_words)\n",
        "\n",
        "# The total number of words\n",
        "total_words = sum(word_counts.values())\n",
        "print(f\"total words: {total_words}\")\n",
        "\n",
        "# Get the frequencies of each word\n",
        "word_frequencies = {}\n",
        "for word, count in word_counts.items():\n",
        "    word_frequencies[word] = count / total_words\n",
        "    \n",
        "# Top 10 word's frequencies\n",
        "sorted_word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "top_10_word_frequencies = sorted_word_frequencies[:10]\n",
        "print(\"Frequencies of 10 most common word\")\n",
        "for word, freq in top_10_word_frequencies:\n",
        "    print(f\"{word}: {freq *100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXFc-u37y2aJ"
      },
      "outputs": [],
      "source": [
        "class StoryGenerator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_grams_size, n_layers=1):\n",
        "        super(StoryGenerator, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_grams_size = n_grams_size - 1\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size*self.n_grams_size,\n",
        "                            hidden_size=hidden_size, \n",
        "                            num_layers=n_layers, \n",
        "                            batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        # Shape: (batch_size, seq_len, hidden_size)\n",
        "        input = self.embedding(input)\n",
        "        # Reshape to (batch_size, seq_len, hidden_size * (g_grams_size - 1))\n",
        "        input = input.view(input.size(0), -1, self.hidden_size * self.n_grams_size)\n",
        "        output, hidden = self.lstm(input, hidden)\n",
        "        # Shape: (batch_size, output_size)\n",
        "        output = self.linear(output[:, -1, :])\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        # LSTM requires tuple as output\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djs8gf62y2aJ"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqJU3E2Py2aJ"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, dataset):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Create dataloader\n",
        "    data_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Disable gradient computation\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_target in data_loader:\n",
        "            hidden_h, hidden_c = model.init_hidden(batch_data.size(0))\n",
        "\n",
        "            output, (hidden_h, hidden_c) = model(batch_data, (hidden_h, hidden_c))\n",
        "            _, predicted = torch.max(output, dim=1)\n",
        "            correct += (predicted == batch_target.squeeze()).sum().item()\n",
        "            total += batch_target.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train(model, train_data, train_target, valid_data, valid_target, num_epochs, learning_rate, batch_size=32, checkpoint_path=None):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Convert the lists of tensors to a single tensor\n",
        "    train_data_tensor = torch.stack(train_data)\n",
        "    train_target_tensor = torch.stack(train_target)\n",
        "\n",
        "    valid_data_tensor = torch.stack(valid_data)\n",
        "    valid_target_tensor = torch.stack(valid_target)\n",
        "\n",
        "    # Create a TensorDataset and DataLoader for mini-batches\n",
        "    train_dataset = TensorDataset(train_data_tensor, train_target_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    valid_dataset = TensorDataset(valid_data_tensor, valid_target_tensor)\n",
        "\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "    n = 0 # nums of iterations\n",
        "    for i in range(0, num_epochs):\n",
        "        hidden_h, hidden_c = model.init_hidden(batch_size)\n",
        "        for batch_num, (batch_train_data, batch_train_target) in enumerate(train_loader):  \n",
        "            # clean up gradient\n",
        "            optimizer.zero_grad()\n",
        "            # forward step\n",
        "            output, (hidden_h, hidden_c) = model(batch_train_data, (hidden_h, hidden_c))\n",
        "            # compute total loss\n",
        "            loss = criterion(output, batch_train_target.squeeze())\n",
        "            # detach hidden_h and hidden_c for next iteration\n",
        "            hidden_h = hidden_h.detach()\n",
        "            hidden_c = hidden_c.detach()\n",
        "            \n",
        "            # compute updates for each parameters and make the update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # clean up gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)\n",
        "            n += 1\n",
        "\n",
        "        train_accuracy = get_accuracy(model, train_dataset)\n",
        "        valid_accuracy = get_accuracy(model, valid_dataset)\n",
        "        train_cost = float(loss)/batch_size\n",
        "\n",
        "        iters_sub.append(n)\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(valid_accuracy)\n",
        "        print('Epoch {}. Iter {}. [Val Acc {}%] [Train Acc {}%, Loss {}]'.format(i+1, n, valid_accuracy * 100, train_accuracy * 100, train_cost))\n",
        "        if checkpoint_path is not None:\n",
        "            torch.save(model.state_dict(), checkpoint_path.format(i))\n",
        "          \n",
        "\n",
        "    # return iters, losses, iters_sub, train_accs, val_accs\n",
        "    plot(iters, losses, iters_sub, train_accs, val_accs)\n",
        "\n",
        "\n",
        "\n",
        "def plot(iters, losses, iters_sub, train_accs, val_accs):\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "vocab_length = len(word_to_index)\n",
        "story_model = StoryGenerator(input_size=vocab_length, hidden_size=hidden_size, output_size=vocab_length, n_layers=n_layers, n_grams_size=n_grams_size)\n",
        "\n",
        "\n",
        "train(story_model,  x_train, y_train, x_valid, y_valid, num_epochs, learning_rate, batch_size=batch_size, checkpoint_path=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h1RjODiy2aK"
      },
      "outputs": [],
      "source": [
        "def generate(model, index_to_word, prompt='family decided spend time next time around city around', length=20, n_grams_size=n_grams_size):\n",
        "    model.eval()\n",
        "    batch_size = 1\n",
        "    hidden_h, hidden_c = story_model.init_hidden(batch_size)\n",
        "    n_grams_size -= 1\n",
        "    for prediction in range(length):\n",
        "        prompt_split = [word_to_index[w] for w in prompt.split()]\n",
        "        prompt_tensor = torch.tensor(prompt_split[-n_grams_size:]).unsqueeze(0)\n",
        "        # Get predictions\n",
        "        output, (hidden_h, hidden_c) = model(prompt_tensor, (hidden_h, hidden_c))\n",
        "        # Make distribution\n",
        "        distribution = output.data.view(-1).exp()\n",
        "        # Sample from distribution\n",
        "        sample = torch.multinomial(distribution, 1)[0].item()\n",
        "        # Search for word in 'reverse' dictionry\n",
        "        predicted_word = index_to_word[sample]\n",
        "        # Add word to prompt\n",
        "        prompt += \" \" + predicted_word\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Generate text based on input prompt\n",
        "print(generate(model=story_model, index_to_word=index_to_word, length=generation_length, n_grams_size=n_grams_size, prompt=prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu788-uRZjn7"
      },
      "outputs": [],
      "source": [
        "# This calculates the BlEU score using the test data set. It uses every 50 entries in the\n",
        "# test set, up to a total of 100 entries, then averages them. We limited this \n",
        "# because there are approximately 10000 entries in the test data set and \n",
        "# calculating and averaging the BLEU score for all of them would take an immense\n",
        "# amount of time(something around 15-16 hours).\n",
        "total_score = 0\n",
        "count = 0\n",
        "for data in x_test[:5000:50]:\n",
        "    text = \" \".join([index_to_word[word.item()] for word in data])\n",
        "    result = generate(model=story_model, index_to_word=index_to_word, length=generation_length, n_grams_size=n_grams_size, prompt=text)\n",
        "    result = result.split(\" \")\n",
        "    score = sentence_bleu(all_sentences, result)\n",
        "    total_score += score\n",
        "    count += 1\n",
        "\n",
        "print(total_score/count)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
